\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in,marginparwidth=2in]{geometry}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

% clean citations
\usepackage{cite}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% text layout - change as needed
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% use adjustwidth environment to exceed text width (see examples in text)
\usepackage{changepage}

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% headrule, footrule and page numbers
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}

% use if you want to put caption to the side of the figure - see example in text
\usepackage{sidecap}

% use for have text wrap around figures
\usepackage{wrapfig}
\usepackage[pscoord]{eso-pic}
\usepackage[fulladjust]{marginnote}
\reversemarginpar

% Adding multirow.
\usepackage{multirow}

% Other required things:
\usepackage{color}
\usepackage{subcaption}
\captionsetup[subfigure]{justification=centering}
\usepackage{amsmath}
\newcommand\code[1]{{\small\texttt{#1}}}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
    \textbf\newline{Choosing the number of principal components for denoising single-cell RNA sequencing data}
}
\newline

% authors go here:
%\\
Aaron Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom \\
\bigskip

\end{flushleft}

\section{Background}
Principal components analysis (PCA) \cite{pearson1901lines,hotelling1933analysis} is widely used for dimensionality reduction in a variety of scientific fields including single-cell RNA sequencing (scRNA-seq) data analysis.
PCA constructs new variables -- i.e., principal components (PCs) -- that are linear functions of the original features, successively maximize variance, and are uncorrelated with each other \cite{jolliffe2016principal}.
The first few PCs explain the most variance in the data and are used as a proxy for the original data set in exploratory analysis.
By compressing the input data into a low-dimensional space, PCA removes much of the random high-dimensional noise and reduces the size of the data.
This improves the resolution of interesting signal (e.g., subpopulations or trajectories in scRNA-seq data) as well as the efficiency of downstream procedures like clustering and visualization.

Practitioners will typically use the first $k$ PCs to obtain a low-dimensional approximation of the original data.
As earlier PCs explain a greater proportion of the variance, taking the first $k$ PCs provides the most effective compression while preserving as much information as possible.
However, the question becomes: what is the best choice of $k$?
If too few PCs are used, we may discard some relevant signal that is only captured in later PCs.
On the other hand, if too many PCs are used, we introduce unnecessary noise and offset the benefits of using PCA in the first place.
Deciding how many PCs to retain is a long-standing topic of study \cite{howard1963empirical,zwick1986comparison},
and depends on whether the aim is to recover the true rank of the signal matrix \cite{zwick1986comparison};
to minimize error compared to the true signal \cite{gavish2014optimal};
or to obtain components that are easily interpreted \cite{franklin1995parallel}.

In this report, we explore a number of computational methods for choosing the number of PCs to retain in scRNA-seq data.
We use a variety of simulations to evaluate different methods in terms of their ability to remove noise and recover the true biological signal.
We demonstrate that the strategies used in existing scRNA-seq analysis software are suboptimal but robust and largely effective.

\section{A brief review of some PC selection methods}

\subsection{Elbow detection}
The scree plot displays the percentage of variance explained by successive PCs.
(We will denote the variance explained by PC $k$ as $\sigma^2_k$, where $\sigma^2_k > \sigma^2_{k+1}$.)
Here, the aim is to visually detect the difference between early PCs that capture structure and later PCs that capture random noise.
A sharp drop from the $k$\textsuperscript{th} to $(k+1)$\textsuperscript{th} PC suggests that most of the structure is captured in the first $k$ PCs \cite{cattell1966scree}.
This manifests as an ``elbow'' in the scree plot at $k$, after which there is a plateau in the percentage of variance explained.
To detect the elbow, we consider a line connecting the first and $n$\textsuperscript{th} PCs, using $n=50$ by default.
The elbow -- and the number of PCs to retain -- is defined as the point on the curve in the scree plot that has the largest perpendicular distance to the line.
This algorithm is relatively robust compared to methods that rely on derivatives.

\subsection{Parallel analysis}
Horn's parallel analysis \cite{horn1965rationale} involves permuting all observations for each feature of the input matrix and performing PCA on this permuted matrix.
This yields a variance explained $\omega^2_k$ for the $k$\textsuperscript{th} PC.
Any PC with a $\sigma^2_k$ (from the original input matrix) comparable to $\omega^2_k$ is considered to be uninteresting, 
as the PC explains no more variance than expected under a random model containing no structure. 
One can visualize this strategy by considering a scree plot and discarding all PCs past the first intersection of the curves for the variances explained from the original and permuted PCAs.

Several definitions of ``comparable'' can be used to define the first uninteresting PC.
The simplest is to remove all PCs past and including the first PC where $\sigma^2_k < \omega^2_k$.
However, this retains too many PCs in noisy datasets where the original and permuted variance-explained curves are similar and intersect slowly.
Another solution is to repeat the permutations many times, and define the threshold as an upper quantile of $\omega^2_k$ for each $k$ \cite{buja1992remarks}.
A PC with $\sigma^2_k$ below this quantile is considered to be feasibly sampled from the distribution of $\omega^2_k$ under a random model, and is subsequently discarded.
We use the 95\textsuperscript{th} percentile, which yields a more conservative estimate of the number of PCs.

\subsection{The Marchenko-Pastur law}
The Marchenko-Pastur law \cite{marchenko1967distribution} specifies the asymptotic distribution of singular values for a large random matrix with independent identically distributed (i.i.d.) entries.
This distribution has a strict upper bound that could be used as a threshold on the number of PCs to retain \cite{shekhar2016comprehensive}.
To justify this, we assume that our input matrix can be written as $Y = T + X$, where $T$ is a $r$-rank matrix of true signal and $X$ is a matrix of i.i.d.\ noise.
We further assume that the first $r$ eigenvectors of the covariance matrix are exactly linear combinations of the $r$ basis vectors of $T$,
i.e., the additional noise due to $X$ does not affect the identification of the true basis vectors.
This means that, once the first $r$ eigenvectors are regressed out of $Y$, the residual matrix is equal to $X$.
Thus, we should take all PCs with singular values greater than the Marchenko-Pastur limit for $X$.

In practice, the limit needs to be adjusted to account for the magnitude of the noise.
This is most simply achieved by scaling the limit by the standard deviation of the noise, 
which is itself estimated by modelling technical variation in the space of the input data \cite{lun2016stepbystep}.
An alternative approach is to fit the Marchenko-Pastur distribution to the observed set of singular values for the input matrix \cite{shekhar2016comprehensive},
adjusting for scaling differences between the observed and theoretical distributions.
However, this requires a complete singular value decomposition (SVD) of the input matrix.
This is time-consuming as it precludes the use of approximate SVD algorithms, so we will not consider it here.

\subsection{The Gavish-Donoho method}
Gavish and Donoho \cite{gavish2014optimal} describe a method for determining the optimal threshold for the singular values.
They consider a matrix containing both true signal and i.i.d.\ noise with a constant standard deviation. 
By using only the first $k$ PCs that have singular values above the optimal threshold, 
we can minimize the mean-squared error (MSE) of the low-rank approximation of the input matrix to the true signal.
The Gavish-Donoho threshold needs to be scaled by an estimate of the standard deviation of the noise,
which can again be obtained by modelling technical noise in the input matrix.

\subsection{Jackstraw}
The \textit{Seurat} package for single-cell RNA-seq data analysis \cite{butler2018integrating} uses the jackstraw procedure \cite{chung2015statistical} for determining whether features are significantly associated with individual PCs.
Briefly, observations for each feature in a randomly chosen subset of features are permuted.
The PCA is performed on the modified matrix and a measure of association is computed for each PC with the permuted observations for the features in the chosen subset.
This is repeated over several iterations to obtain a null distribution of association measures.
The association measure is also computed for each feature and PC using the original data.
A $p$-value is subsequently obtained for each gene and PC by comparing its original association measure with that of the null distribution.

In practice, this approach yields a feature-by-PC matrix of $p$-values that needs to be consolidated into a decision regarding the number of PCs to retain. 
We do so by combining the $p$-values for each PC using Simes' method \cite{simes1986improved}.
This yields a combined $p$-value representing the evidence against the global null hypothesis for that PC, 
i.e., that it is not significantly associated with any features.
We then retain PCs up to the first PC that accepts the global null at a error threshold of 0.05.
We note that a variety of other strategies can be used to consolidate multiple $p$-values into a single decision.
However, Simes' method is appealing as it is robust to correlations between features \cite{sarkar1997simes}.

\subsection{Summation of technical components}
The \code{denoisePCA} function in the \textit{scran} package \cite{lun2016stepbystep} defines the number of PCs to retain based on the total technical noise in the input data.
Technical noise is expected to be random and uncorrelated across genes, and thus should be mostly present in the later PCs.
The number of PCs to retain is chosen as the smallest value $l$ such that 
\[
    \sum_{k=l+1}^N \sigma^2_k  \le \omega^2_t \;,
\]
where $N$ is the total number of PCs and $\omega^2_t$ is the sum of technical components for all features, estimated as previously described.
This discards later PCs until the discarded variance is equal to the total technical variance in the data.
The value of $l$ is a lower bound on the number of PCs required to capture the biological signal.
Equality to this lower bound occurs when the signal is wholly captured by the first few PCs,
though in practice, $l$ will be lower than the rank of the matrix containing the signal.
Furthermore, we only use features where the total variance is greater than the estimated technical component.
This ensures that there is some $l \in [1, N]$ that satisfies the above inequality.

\section{Assessing accuracy for any number of PCs}

\subsubsection{MSE of the low-rank approximation}
Consider an input scRNA-seq matrix $\mathbf{Y}$ where the rows are the samples (i.e., cells) and the columns are the features (i.e., genes).
We perform an SVD to obtain the usual $\mathbf{Y} = \mathbf{U}\mathbf{D}\mathbf{V}'$, noting that each column of $\mathbf{U}\mathbf{D}$ is a PC.
Now, assume that we only want to use the first $k$ PCs.
This is equivalent to setting the singular values to zero for all but the first $k$ diagonal entries of $\mathbf{D}$ (denote this modified matrix as $\mathbf{D}_{(k)}$),
which ensures that all but the first $k$ columns of $\mathbf{U}\mathbf{D}$ are zero and can be ignored.
From the Eckart-Young theorem, the best $k$-rank approximation to $\mathbf{Y}$ is $\mathbf{\tilde Y}_{(k)} =  \mathbf{U}\mathbf{D}_{(k)}\mathbf{V}'$.

We further assume that $\mathbf{Y}$ is the sum of a signal matrix $\mathbf{T}$ of the same dimensions, 
containing the true signal (i.e., gene expression) in each entry of the matrix;
and a noise matrix of the same dimensions, where each entry has an expectation of zero and is independently distributed from the other entries.
Our aim is to choose $k$ such that the Frobenius norm of $\mathbf{\tilde Y}_{(k)} - \mathbf{T}$ is minimized.
In other words, we want to minimize the MSE of the low-rank approximation $\mathbf{\tilde Y}_{(k)}$ from the true signal $\mathbf{T}$.
This is arguably the only relevant measure of performance when PCA is applied to remove noise.
We do not intend to interpret the individual PCs, nor do we care about the true rank of $\mathbf{T}$.

It is worth pointing out that the Euclidean distances between cells using the PC coordinates (i.e., from $\mathbf{U}\mathbf{D}_{(k)}$) are the same as the corresponding distances computed from $\mathbf{\tilde Y}_{(k)}$.
This means that minimizing the MSE of $\mathbf{\tilde Y}_{(k)}$ will also improve the correspondence between the distances between cells computed from the PCs and the true distances computed from the unknown $\mathbf{T}$.
Many downstream procedures for scRNA-seq data analysis depend primarily on the distances between cells, e.g., hierarchical clustering, nearest neighbour searches.
Thus, a low MSE in the low-rank approximation is directly relevant to the quality of analyses based on the PCs.

We can also view the choice of $k$ as a compromise between bias and precision.
At the largest value of $k$, there is no bias as $E(\mathbf{\tilde Y}_{(k)}) = E(\mathbf{Y}) = \mathbf{T}$, but precision is poor due to the presence of high-dimensional noise.
As $k$ decreases, noise is removed and precision improves.
However, this comes at the cost of introducing more bias when PCs capturing aspects of $\mathbf{T}$ are discarded.
The MSE is the sum of the bias $||E(\mathbf{\tilde Y}_{(k)}) - \mathbf{T}||_F$ and the variance $||\tilde Y_{(k)} - E(\mathbf{\tilde Y}_{(k)})||_F$, and provides a single measure for evaluating accuracy.

\bibliography{ref}
\bibliographystyle{unsrt}
\end{document}
