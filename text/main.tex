\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in,marginparwidth=2in]{geometry}

% use Unicode characters - try changing the option if you run into troubles with special characters (e.g. umlauts)
\usepackage[utf8]{inputenc}

% clean citations
\usepackage{cite}

% hyperref makes references clicky. use \url{www.example.com} or \href{www.example.com}{description} to add a clicky url
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

% improves typesetting in LaTeX
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% text layout - change as needed
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Remove % for double line spacing
%\usepackage{setspace} 
%\doublespacing

% use adjustwidth environment to exceed text width (see examples in text)
\usepackage{changepage}

% adjust caption style
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,singlelinecheck=off]{caption}

% remove brackets from references
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% headrule, footrule and page numbers
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}

% use \textcolor{color}{text} for colored text (e.g. highlight to-do areas)
\usepackage{color}

% define custom colors (this one is for figure captions)
\definecolor{Gray}{gray}{.25}

% this is required to include graphics
\usepackage{graphicx}

% use if you want to put caption to the side of the figure - see example in text
\usepackage{sidecap}

% use for have text wrap around figures
\usepackage{wrapfig}
\usepackage[pscoord]{eso-pic}
\usepackage[fulladjust]{marginnote}
\reversemarginpar

% Adding multirow.
\usepackage{multirow}

% Other required things:
\usepackage{color}
\usepackage{subcaption}
\captionsetup[subfigure]{justification=centering}
\usepackage{amsmath}
\newcommand\code[1]{{\small\texttt{#1}}}

% document begins here
\begin{document}
\vspace*{0.35in}

% title goes here:
\begin{flushleft}
{\Large
    \textbf\newline{Choosing the number of principal components for denoising single-cell RNA sequencing data}
}
\newline

% authors go here:
%\\
Aaron Lun\textsuperscript{1,*}
\\
\bigskip
\bf{1} Cancer Research UK Cambridge Institute, University of Cambridge, Li Ka Shing Centre, Robinson Way, Cambridge CB2 0RE, United Kingdom \\
\bigskip

\end{flushleft}

\section{Background}
Principal components analysis (PCA) \cite{pearson1901lines,hotelling1933analysis} is widely used for dimensionality reduction in a variety of scientific fields including single-cell RNA sequencing (scRNA-seq) data analysis.
PCA constructs new variables -- i.e., principal components (PCs) -- that are linear functions of the original features, successively maximize variance, and are uncorrelated with each other \cite{jolliffe2016principal}.
The first few PCs explain the most variance in the data and are used as a proxy for the original data set in exploratory analysis.
By compressing the input data into a low-dimensional space, PCA removes much of the random high-dimensional noise and reduces the size of the data.
This improves the resolution of interesting signal (e.g., subpopulations or trajectories in scRNA-seq data) as well as the efficiency of downstream procedures like clustering and visualization.

Practitioners will typically use the first $k$ PCs to obtain a low-dimensional approximation of the original data.
As earlier PCs explain a greater proportion of the variance, taking the first $k$ PCs provides the most effective compression while preserving as much information as possible.
However, the question becomes: what is the best choice of $k$?
If too few PCs are used, we may discard some relevant signal that is only captured in later PCs.
On the other hand, if too many PCs are used, we introduce unnecessary noise and offset the benefits of using PCA in the first place.
Deciding how many PCs to retain is a long-standing topic of study \cite{howard1963empirical,zwick1986comparison},
and depends on whether the aim is to recover the true rank of the signal matrix \cite{zwick1986comparison};
to minimize error compared to the true signal \cite{gavish2014optimal};
or to obtain components that are easily interpreted \cite{franklin1995parallel}.

In this report, we explore a number of computational methods for choosing the number of PCs to retain in scRNA-seq data.
We use a variety of simulations to evaluate different methods in terms of their ability to remove noise and recover the true biological signal.
We demonstrate that the strategies used in existing scRNA-seq analysis software are suboptimal but robust and largely effective.

\section{A brief review of some PC selection methods}

\subsection{Elbow detection}
The scree plot displays the percentage of variance explained by successive PCs.
(We will denote the variance explained by PC $k$ as $\sigma^2_k$, where $\sigma^2_k > \sigma^2_{k+1}$.)
Here, the aim is to visually detect the difference between early PCs that capture structure and later PCs that capture random noise.
A sharp drop from the $k$\textsuperscript{th} to $(k+1)$\textsuperscript{th} PC suggests that most of the structure is captured in the first $k$ PCs \cite{cattell1966scree}.
This manifests as an ``elbow'' in the scree plot at $k$, after which there is a plateau in the percentage of variance explained.
To detect the elbow, we consider a line connecting the first and $n$\textsuperscript{th} PCs, using $n=50$ by default.
The elbow is defined as the point on the curve in the scree plot that has the largest perpendicular distance to the line.
We then retain all PCs up to the elbow point (but not including) the elbow point.
This algorithm is relatively robust compared to derivative-based methods that are sensitive to unstable numerical differentiation.

\subsection{Parallel analysis}
Horn's parallel analysis \cite{horn1965rationale} involves permuting all observations for each feature of the input matrix and performing PCA on this permuted matrix.
This yields a variance explained $\omega^2_k$ for the $k$\textsuperscript{th} PC.
Any PC with a $\sigma^2_k$ (from the original input matrix) comparable to $\omega^2_k$ is considered to be uninteresting, 
as the PC explains no more variance than expected under a random model containing no structure. 
One can visualize this strategy by considering a scree plot and discarding all PCs past the first intersection of the curves for the variances explained from the original and permuted PCAs.

Several definitions of ``comparable'' can be used to define the first uninteresting PC.
The simplest is to remove all PCs past and including the first PC where $\sigma^2_k < \omega^2_k$.
However, this retains too many PCs in noisy datasets where the original and permuted variance-explained curves are similar and intersect slowly.
Another solution is to repeat the permutations many times, and define the threshold as an upper quantile of $\omega^2_k$ for each $k$ \cite{buja1992remarks}.
A PC with $\sigma^2_k$ below this quantile is considered to be feasibly sampled from the distribution of $\omega^2_k$ under a random model, and is subsequently discarded.
We use the 95\textsuperscript{th} percentile, which yields a more conservative estimate of the number of PCs.

\subsection{The Marchenko-Pastur law}
The Marchenko-Pastur law \cite{marchenko1967distribution} specifies the asymptotic distribution of singular values for a large random matrix with independent identically distributed (i.i.d.) entries.
This distribution has a strict upper bound that could be used as a threshold on the number of PCs to retain \cite{shekhar2016comprehensive}.
To justify this, we assume that our input matrix can be written as $\mathbf{Y} = \mathbf{T} + \mathbf{X}$, where $\mathbf{T}$ is a $r$-rank matrix of true signal and $\mathbf{X}$ is a matrix of i.i.d.\ noise.
We further assume that the first $r$ rotation vectors are exactly linear combinations of the $r$ basis vectors of $\mathbf{T}$,
i.e., the additional noise due to $\mathbf{X}$ does not affect the identification of the true basis vectors.
This means that the difference between $\mathbf{Y}$ and the low-rank approximation derived from the first $r$ PCs will be equal to $\mathbf{X}$.
Thus, we should take all PCs with singular values greater than the Marchenko-Pastur limit for $\mathbf{X}$.

In practice, the limit needs to be adjusted to account for the magnitude of the noise.
This is most simply achieved by scaling the limit by the standard deviation of the noise, 
which is itself estimated by modelling technical variation in the space of the input data \cite{lun2016stepbystep}.
An alternative approach is to fit the Marchenko-Pastur distribution to the observed set of singular values for the input matrix \cite{shekhar2016comprehensive},
adjusting for scaling differences between the observed and theoretical distributions.
However, this requires a complete singular value decomposition (SVD) of the input matrix.
This is time-consuming as it precludes the use of approximate SVD algorithms, so we will not consider it here.

\subsection{The Gavish-Donoho method}
Gavish and Donoho \cite{gavish2014optimal} describe a method for determining the optimal threshold for the singular values.
They consider a matrix containing both true signal and i.i.d.\ noise with a constant standard deviation. 
By using only the first $k$ PCs that have singular values above the optimal threshold, 
we can minimize the mean-squared error (MSE) of the low-rank approximation of the input matrix to the true signal.
The Gavish-Donoho threshold needs to be scaled by an estimate of the standard deviation of the noise,
which can again be obtained by modelling technical noise in the input matrix.

\subsection{Jackstraw}
The \textit{Seurat} package for single-cell RNA-seq data analysis \cite{butler2018integrating} uses the jackstraw procedure \cite{chung2015statistical} for determining whether features are significantly associated with individual PCs.
Briefly, observations for each feature in a randomly chosen subset of features are permuted.
The PCA is performed on the modified matrix and a measure of association is computed for each PC with the permuted observations for the features in the chosen subset.
This is repeated over several iterations to obtain a null distribution of association measures.
The association measure is also computed for each feature and PC using the original data.
A $p$-value is subsequently obtained for each gene and PC by comparing its original association measure with that of the null distribution.

In practice, this approach yields a feature-by-PC matrix of $p$-values that needs to be consolidated into a decision regarding the number of PCs to retain. 
We do so by combining the $p$-values for each PC using Simes' method \cite{simes1986improved}.
This yields a combined $p$-value representing the evidence against the global null hypothesis for that PC, 
i.e., that it is not significantly associated with any features.
We then retain PCs up to the first PC that accepts the global null at a error threshold of 0.05.
We note that a variety of other strategies can be used to consolidate multiple $p$-values into a single decision.
However, Simes' method is appealing as it is robust to correlations between features \cite{sarkar1997simes}.

\subsection{Summation of technical components}
The \code{denoisePCA} function in the \textit{scran} package \cite{lun2016stepbystep} defines the number of PCs to retain based on the total technical noise in the input data.
Technical noise is expected to be random and uncorrelated across genes, and thus should be mostly present in the later PCs.
The number of PCs to retain is chosen as the smallest value $l$ such that 
\[
    \sum_{k=l+1}^N \sigma^2_k  \le \omega^2_t \;,
\]
where $N$ is the total number of PCs and $\omega^2_t$ is the sum of technical components for all features, estimated as previously described.
This discards later PCs until the discarded variance is equal to the total technical variance in the data.
The value of $l$ is a lower bound on the number of PCs required to capture the biological signal.
Equality to this lower bound occurs when the signal is wholly captured by the first few PCs,
though in practice, $l$ will be lower than the rank of the matrix containing the signal.
Furthermore, we only use features where the total variance is greater than the estimated technical component.
This ensures that there is some $l \in [1, N]$ that satisfies the above inequality.

\section{Assessing accuracy with the MSE}
Consider an input scRNA-seq matrix $\mathbf{Y}$ where the rows are the samples (i.e., cells) and the columns are the features (i.e., genes).
We perform an SVD to obtain the usual $\mathbf{Y} = \mathbf{U}\mathbf{D}\mathbf{V}'$, noting that each column of $\mathbf{U}\mathbf{D}$ is a PC.
Now, assume that we only want to use the first $k$ PCs.
This is equivalent to setting the singular values to zero for all but the first $k$ diagonal entries of $\mathbf{D}$ (denote this modified matrix as $\mathbf{D}_{(k)}$),
which ensures that all but the first $k$ columns of $\mathbf{U}\mathbf{D}$ are zero and can be ignored.
From the Eckart-Young theorem, the best $k$-rank approximation to $\mathbf{Y}$ is $\mathbf{\tilde Y}_{(k)} =  \mathbf{U}\mathbf{D}_{(k)}\mathbf{V}'$.

We further assume that $\mathbf{Y}$ is the sum of a signal matrix $\mathbf{T}$ of the same dimensions, 
containing the true signal (i.e., gene expression) in each entry of the matrix;
and a noise matrix of the same dimensions, where each entry has an expectation of zero and is independently distributed from the other entries.
Our aim is to choose $k$ such that the Frobenius norm of $\mathbf{\tilde Y}_{(k)} - \mathbf{T}$ is minimized.
In other words, we want to minimize the MSE of the low-rank approximation $\mathbf{\tilde Y}_{(k)}$ from the true signal $\mathbf{T}$.
This is arguably the only relevant measure of performance when PCA is applied to remove noise.
We do not intend to interpret the individual PCs, nor do we care about the true rank of $\mathbf{T}$.

It is worth pointing out that the Euclidean distances between cells using the PC coordinates (i.e., from $\mathbf{U}\mathbf{D}_{(k)}$) are the same as the corresponding distances computed from $\mathbf{\tilde Y}_{(k)}$.
This means that minimizing the MSE of $\mathbf{\tilde Y}_{(k)}$ will also improve the correspondence between the distances between cells computed from the PCs and the true distances computed from the unknown $\mathbf{T}$.
Many downstream procedures for scRNA-seq data analysis depend primarily on the distances between cells, e.g., hierarchical clustering, nearest neighbour searches.
Thus, a low MSE in the low-rank approximation is directly relevant to the quality of analyses based on the PCs.

We can also view the choice of $k$ as a compromise between bias and precision.
At the largest value of $k$, there is no bias as $E(\mathbf{\tilde Y}_{(k)}) = E(\mathbf{Y}) = \mathbf{T}$, but precision is poor due to the presence of high-dimensional noise.
As $k$ decreases, noise is removed and precision improves.
However, this comes at the cost of introducing more bias when PCs capturing aspects of $\mathbf{T}$ are discarded.
The MSE is the sum of the bias $||E(\mathbf{\tilde Y}_{(k)}) - \mathbf{T}||^2_F$ and the variance $||\mathbf{\tilde Y}_{(k)} - E(\mathbf{\tilde Y}_{(k)})||^2_F$, and provides a single overall measure of accuracy.

\section{Evaluations with simple simulations}

\subsection{Simulation design}
We first considered a simple simulation involving $S$ subpopulations of $C$ cells with $G$ genes.
A proportion of genes $P$ were chosen to drive biological heterogeneity.
For each gene $g$ in the chosen set, the mean expression $\mu_{gp}$ in subpopulation $p$ was randomly sampled from a Normal distribution with mean zero and variance $s^2$.
For the remaining genes, the mean expression was set to zero for all subpopulations.
Each cell was randomly assigned to one subpopulation.
The observed expression for gene $g$ in cell $c$ assigned to $p$ was defined as $\mu_{gp} + \epsilon_{gc}$ where $\epsilon_{gc}$ was randomly sampled from a Normal distribution with mean zero and variance $w^2_g$.
We tested all combinations of parameters for $S$ from 5 to 20; $s^2$ from 0.2 to 1; $P$ from 0.2 to 1; $C$ from 200 to 5000; $G$ from 1000 to 5000;
and $w^2_g$ of 1 (i.e., no heteroskedasticity) or sampled from a $\mbox{Gamma}(2,2)$ distribution (moderate) or $\mbox{Gamma}(0.2,0.2)$ (high).
We also repeated the simulations where, instead of assigning each cell to a single subpopulation to create clusters,
we created trajectories by considering each cell as a linear combination of two randomly chosen subpopulations with mixing proportion sampled from $\mbox{Uniform}(0, 1)$.

We evaluated each choice of the number of PCs by computing the MSE of the low-rank approximation $\mathbf{\tilde Y}_{(k)}$ from the $\mathbf{T}$ matrix containing the known signal for each cell.
For each cell, entries of $\mathbf{T}$ were set to the mean expression vector of the assigned subpopulation (i.e., $\mu_{gp}$) for the cluster simulations.
For trajectories, entries of $\mathbf{T}$ were defined as the known linear combination of the two chosen subpopulations for each cell.
In each simulation scenario, we determined lowest MSE obtained by testing all possible values of $k$.
We then reported the results for each method in terms of fold-increases from the optimal MSE.
For the technical summation method, we estimated $\omega^2_t$ as $||\mathbf{Y} - \mathbf{T}||^2_F$.
(In practice, $\mathbf{T}$ is neither known nor necessary for estimating the technical components, but we have used it here for simplicity.)
For the Gavish-Donoho and Marchenko-Pastur methods, the average variance of the noise was defined as $\omega^2/G$.

\subsection{Simulation results}
Across all simulation scenarios, technical summation performs consistently well with MSEs close to or at the optimum (Figure~\ref{fig:all}). 
The jackstraw method also performs well, recovering the optimal MSE more frequently than summation but exhibiting a heavier tail of large MSEs.
The different nature of the suboptimalities reflects the theoretical differences between the two approaches.
Technical summation consistently underestimates the true rank of $\mathbf{T}$, which likely contributes to a systematic shift from the optimal MSE in some scenarios.
The jackstraw involves random permutations and relies on $p$-values that are inherently stochastic, which increases the variation in the chosen number of PCs across iterations.
Parallel analysis recovers the optimal MSE most often but has an even heavier right tail than the jackstraw.
This is attributable to increased variation in $k$ due to the randomness of permuting an entire matrix.

\begin{figure}
\centering
\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip,page=1]{../simulations/pics/Mode.pdf}
    \caption{}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip,page=2]{../simulations/pics/Mode.pdf}
    \caption{}
\end{subfigure}
\caption{Distributions of MSEs for the number of PCs chosen by each method, shown as fold increases from the optimal MSE.
Results were obtained for 10 simulation iterations for each combination of parameters for the (a) cluster or (b) trajectory simulations.
For each method, the dark grey bar on the left represents the percentage of iterations in which the observed MSE was equal to the optimal value,
while the light grey histogram shows the distribution of all MSEs that were greater than the optimum.}
\label{fig:all}
\end{figure}

The performance of the Gavish-Donoho method warrants some closer inspection.
When the variance of the noise $w^2_g$ is constant, this method is almost perfect with respect to minimizing the MSE (Figure~\ref{fig:noise}a).
This is consistent with the theoretical guarantees on its performance \cite{gavish2014optimal}.
When $w^2_g$ varies across genes, the MSE increases above that of other methods (Figure~\ref{fig:noise}b, c).
This reflects the sensitivity of the method to violations of the assumption of a constant variance for the noise,
which is unfortunate as (sc)RNA-seq data often exhibits strong mean-variance relationships \cite{lun2016stepbystep,law2014voom}. 
It is possible to mitigate violations of this assumption by scaling the input matrix so that the technical components are the same across all genes.
However, scaling would also distort the magnitudes of the biological components, e.g., by upweighting stably expressed high-abundance genes with small technical components.
This changes the nature of the PCA and obviously precludes the recovery of the correct $\mathbf{T}$.

\begin{figure}
\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 5mm,clip,page=3]{../simulations/pics/Noise.pdf}
    \caption{}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 5mm,clip,page=2]{../simulations/pics/Noise.pdf}
    \caption{}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 5mm,clip,page=1]{../simulations/pics/Noise.pdf}
    \caption{}
\end{subfigure}
\caption{Distributions of MSEs for the number of PCs chosen by each method, shown as fold-increases from the optimal MSE.
Results were obtained for 10 simulation iterations for each combination of parameters with 
(a) no variability in gene-wise variances, i.e., $w_g^2=1$;
(b) moderate variability, i.e., $w_g^2 \sim \mbox{Gamma}(2, 2)$;
or (c) high variability, i.e., $w_g^2 \sim \mbox{Gamma}(0.2, 0.2)$.
Bars and histograms are as described in Figure~\ref{fig:all}.    
}
\label{fig:noise}
\end{figure}

The use of the Marchenko-Pastur limit also exhibits poor performance, which can be traced back to the underying assumptions.
Like the Gavish-Donoho method, the application of the Marchenko-Pastur law requires the assumption of i.i.d.\ noise.
Violations of this assumption lead to increased MSE when the limit is used to choose number of PCs (Figure~\ref{fig:noise}).
However, even in the absence of variable noise, we observe some inflated MSEs compared to the other methods.
This is because the $r$ basis vectors of $\mathbf{T}$ will rarely be well-represented by the first $r$ rotation vectors in noisy datasets.
Instead, some of the variance in the first $r$ basis vectors will be captured by the $>r$ rotation vectors.
This increases the later singular values above the Marchenko-Pastur limit and generally results in an overstatement of the number of PCs to retain. 

\section{Evaluations with real data-based simulations}

\subsection{Simulation design}
We generated log-normalized expression values from real scRNA-seq data by following a published workflow \cite{lun2016stepbystep}.
We first removed low-quality cells with low total counts, low total numbers of detected features and high spike-in proportions \cite{mccarthy2017scater}.
We computed size factors for each gene using the deconvolution method \cite{lun2016pooling}, and separate size factors for each spike-in transcript \cite{lun2017assessing}.
We divided the counts by the relevant size factors to obtain log-transformed normalized expression values.
We estimated the technical component of variance of the transformed data for each gene by fitting a mean-dependent trend to the spike-in variances.
We performed feature selection by only retaining endogenous genes with total variance greater than the technical component.
If spike-in transcripts were not available, we instead used mitochondrial proportions for quality control,
and we estimated the technical component for each gene by assuming Poisson noise.  

Given a log-normalized expression matrix $\mathbf{Z}$, we generated a $s$-rank approximation $\mathbf{\tilde Z}_{(s)}$ as described above.
For the purpose of simulating data, we treated $\mathbf{\tilde Z}_{(s)}$ as the matrix of true signal $\mathbf{T}$.
We then simulated the observed matrix $\mathbf{Y}$ by computing $\mathbf{S} \circ (\mathbf{\tilde Z}_{(s)} - \mathbf{Z})$,
where $\mathbf{S}$ is a random sign matrix of the same dimensions as $\mathbf{Z}$, i.e., each entry of $\mathbf{S}$ is independently sampled from $\{-1, 1\}$.
This framework allows us to incorporate aspects of real biological structure from $\mathbf{Z}$ into the simulated data $\mathbf{Y}$.
By only changing the sign of the residuals $\mathbf{\tilde Z}_{(s)} - \mathbf{Z}$, we also avoid making strong assumptions about the distribution of the noise.
In particular, we can recapitulate the effects of discreteness and gene-to-gene heteroskedasicity more accurately.
Obviously, the true rank of $\mathbf{Z}$ is not known, so we repeated these simulations using $s$ from 10 to 30.
Methods for choosing $k$ were assessed as described for the simple simulations.

%ensures that $E(\mathbf{Y}) = \mathbf{\tilde Z}_{(s)}$;

\bibliography{ref}
\bibliographystyle{unsrt}
\end{document}
